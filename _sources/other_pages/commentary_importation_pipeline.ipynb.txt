{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Commentary importation pipeline\n",
    "\n",
    "This notebook goes through all the steps involved in the creation of `CanonicalCommentary`s from OCR outputs.\n",
    "\n",
    "We will therefore:\n",
    "\n",
    "1. See how to import an `RawCommentary` from OCR outputs.\n",
    "2. See how to optimise this commentary and transform it to a `CanonicalCommentary`\n",
    "3. See how to export it to a canonical json format for later use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating an `OcrCommetary`.\n",
    "\n",
    "`RawCommentary`s need access to (at least) three kind of information:\n",
    "- OCR outputs files, which represent single pages and which will serve as a basis to create `RawPage` objects\n",
    "- The corresponding images (after which the former are named)\n",
    "- A via-project.json containing information about the layout.\n",
    "\n",
    "Using the data provided in `ajmc/data/sample_commentaries`, we can create our first `RawCommentary`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from ajmc.text_processing.raw_classes import RawCommentary\n",
    "from ajmc.commons import variables as vs\n",
    "\n",
    "comm_id = 'sophoclesplaysa05campgoog'\n",
    "ocr_run_id = '3467O2_tess_retrained'\n",
    "\n",
    "ocr_commentary = RawCommentary(id=comm_id,\n",
    "                               ocr_dir=vs.get_comm_ocr_outputs_dir(comm_id, ocr_run_id),\n",
    "                               via_path=vs.get_comm_via_path(comm_id),\n",
    "                               image_dir=vs.get_comm_img_dir(comm_id))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T13:24:31.182665Z",
     "start_time": "2023-09-03T13:24:30.432138Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Providing all these paths can be cumbersome. `ajmc` therefore has a systematic directory structure (see `ajmc/notebooks/data_organisation.ipynb`) which allows us to create a commentary directly from its OCR outputs directory if it is compliant with the project\\'s folder structure. As `../data/sample_commentaries` are ajmc-compliant, we can simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Note that our path holds to the structure pattern : '/abspath/to/root_dir/[comm_id]/ocr/runs/[ocr_run_id]/outputs'\n",
    "ocr_commentary = RawCommentary.from_ajmc_data(id=comm_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T13:25:02.571818Z",
     "start_time": "2023-09-03T13:25:02.563701Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The creation of an `RawCommentary` will take care of the creation of its pages, lines, regions and words. However, it is also possible to instantiate any of these directly:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from ajmc.text_processing.ocr_classes import RawPage\n",
    "\n",
    "\n",
    "page = RawPage(ocr_path=(vs.get_comm_ocr_outputs_dir(comm_id, ocr_run_id) / 'sophoclesplaysa05campgoog_0148.hocr'),\n",
    "               image_path=vs.get_comm_img_dir(comm_id) / 'sophoclesplaysa05campgoog_0148.png',\n",
    "               commentary=ocr_commentary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T13:26:07.050065Z",
     "start_time": "2023-09-03T13:26:07.041194Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note:\n",
    "    It is not necessary to provide all the arguments provided here. For instance, if you leave `commentary=...` blank, the object will still be functionnal, but you won't be able to retrieve commentary-level information, such as the via_project."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Why should one bother creating `CanonicalCommentary`s ? \n",
    "\n",
    "- TL;DR : Skip to the next section if you don't care about the details.\n",
    "\n",
    "### The vagaries of OCR\n",
    "\n",
    "You may ask yourself: what's actually the problem with `RawCommentary`s ? Why should we care about enhancing an `RawCommentary` in the first place ? Well, the problem is not really about the object itself but on the many inconsistencies and noise of the OCR outputs it relies on. To cite a few:\n",
    "    1. Empty or non words\n",
    "    2. Crummy, elongated, stretched or shrinked word bounding boxes or even inverted bounding boxes with negative width and height.\n",
    "    3. Labyrinthine reading order (very often due to marginal line numbers)\n",
    "    4. Single lines spanning over multiple columns, multiple lines or side numbers\n",
    "    5. Diacritics recognized as single lines\n",
    "    6. Crummy, elongated, stretched or shrinked line bounding boxes\n",
    "    7. ...\n",
    "\n",
    "\n",
    "### The weakness of xml formats\n",
    "\n",
    "To add to this already long though not exhaustive list of pitfalls, one should add two other caveats:\n",
    "- OCR outputs come in different formats (Kraken or Tesseract style `hocr`, `alto`, `xml`...)\n",
    "- Though very different because of their individualistic wills to create [harmonized, overarching standards](https://xkcd.com/927/), these formats all share the same weakness: the nested architecture of xml-like documents. This property alone makes xml like formats inadequate to our purposes. Let me provided with a simple example. Say we have the following page :\n",
    "\n",
    "```xml\n",
    "<xml_page attribute_1=\"...\" attribute_2=\"...\">\n",
    "    <xml_line attribute_1=\"...\" attribute_2=\"...\">\n",
    "        <xml_word attribute_1=\"...\" attribute_2=\"...\">Foo</xml_word>\n",
    "        <xml_word attribute_1=\"...\" attribute_2=\"...\">Bar</xml_word>\n",
    "    </xml_line>\n",
    "    <xml_line attribute_1=\"...\" attribute_2=\"...\">\n",
    "        <xml_word attribute_1=\"...\" attribute_2=\"...\">Zig</xml_word>\n",
    "        <xml_word attribute_1=\"...\" attribute_2=\"...\">Zag</xml_word>\n",
    "    </xml_line>\n",
    "</xml_page>\n",
    "```\n",
    "In `xml_page` we have two `xml_line` elements, which themselves contain two `xml_word` elements. This may already be complicated to navigate through, but the most vicious issue is still to come. It appears when you try to overlap different layers of text containers. Say you have a region spanning only the `n` first word of a line. Should your region be a child of the line ? This makes no sense from a global perspective: regions (such as paragraphs) are higher in the hierarchy and should be parent to lines. One could be tempted to create a line for the region, but then an other problem arises: when calling all the lines from a page, should one call the lines from the regions or from the lines directly, as they are now different ? The same problem appears with entities (e.g. named entities) that span over multiple pages. Say we have an entity starting with the two last words of the last line of page `n` and ends with the first word of the main text of page `n+1`. Retrieve the words in such an entity demands extrem precision and absurdly complex chunks of code. In pseudo-python, you would end up with something like `my_entity.words = pages[n].children.lines[-1].children.words[-2:]+page[n+1].children.lines[0].children.words[0]`. And this is even yet a simple case.  What if you have a footnote on page `n` that you don't want to include ? What if the first line of page `n+1` is actually the page number and not the main text ? I let you imagine the kind of recondite code you end up with (`my_entity.words = pages[n].children.find_all(\"regions\", type=\"main_text\")[-1].children.lines[-1].words[-2:]+pages[n+1]...`). Not even mentionning the fact that this code is not yet dynamic and that a simple change in page numbering, word alignment or region reading order completely ruins the pipeline.\n",
    "\n",
    "### The advantages of the canonical format\n",
    "\n",
    "To tackle these issues, we propose with a fairly simple canonical format to store our data. The philosophy of its implementation is to go from a hierarchised and nested structure to a fully horizontal structure. Instead of having nested and re-nested text containers we collect a global list of words and map every other text container to a word range. Here's an example\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"words\" : [{\"text\":\"Foo\", \"attribute_1\":\"...\", \"attribute_2\":\"...\"},\n",
    "             {\"text\":\"Bar\", \"attribute_1\": \"...\", \"attribute_2\":\"...\"},\n",
    "             {\"text\":\"Zig\", \"attribute_1\": \"...\", \"attribute_2\":\"...\"},\n",
    "             {\"text\":\"Zag\", \"attribute_1\": \"...\", \"attribute_2\":\"...\"}],\n",
    "  \"pages\": [{\"word_range\": [0,3]}],\n",
    "  \"lines\": [{\"word_range\": [0,1]},\n",
    "            {\"word_range\": [2,3]}]\n",
    "}\n",
    "```\n",
    "\n",
    "This format comes with a lot of advantages :\n",
    "\n",
    "1. It's a `json`, not an `xml`. It's easily readable both by humans and machines. You can import it as a python `dict` in 2 lines of code. No need for more complex `bs4` or `etree` objects that would clearly overkill for our purposes and offer nothing that `json`s or `dict`s can't do.\n",
    "2. It solves the nesting and overlapping problem at once. You can have overlapping, nested, renested textcontainers. Important thing is that they can be accessed **horizontally**, simply by finding the other textcontainers with included or overlapping word ranges. Same to get a textcontainer's words : simply call `my_tc.words = words[*my_tc.word_range]`.\n",
    "3. It makes redundant information of xmls useless: To get a line's bounding box, you simply concatenate it's words bounding box. This allows to store an entire 400 pages commentary in a ~35MB file, as opposed to ~85MB other OCR outputs (with no information loss and no optimisation), which transitions well to the next point.\n",
    "4.  It is computationnaly efficient. See a simple example here :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring RawCommentary importation time and manipulation time\n",
      "    Time required by importation and word retrieval: 9.01s\n",
      "    Time required to retrieve the text lines containing decimals: 2.03s\n",
      "\n",
      "Measuring CanonicalCommentary importation time and manipulation time\n",
      "    Time required by importation and word retrieval: 3.94s\n",
      "    Time required to retrieve the text lines containing decimals: 0.23s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "from ajmc.text_processing.ocr_classes import RawCommentary\n",
    "from ajmc.text_processing.canonical_classes import CanonicalCommentary\n",
    "\n",
    "\n",
    "\n",
    "def time_commentary_operations(commentary_type):\n",
    "    print(f'Measuring {commentary_type} importation time and manipulation time')\n",
    "\n",
    "    json_path = vs.COMMS_DATA_DIR / 'sophoclesplaysa05campgoog/canonical/3467O2_tess_retrained.json'\n",
    "    \n",
    "    start_time = time.time()\n",
    "    if commentary_type == \"RawCommentary\":\n",
    "        commentary = RawCommentary.from_ajmc_data(comm_id)\n",
    "    else:\n",
    "        commentary = CanonicalCommentary.from_json(json_path)\n",
    "\n",
    "    commentary.children.words\n",
    "    print(\"    Time required by importation and word retrieval: {:.2f}s\".format(time.time() - start_time))\n",
    "\n",
    "    start_time = time.time()\n",
    "    [l.text for l in commentary.children.lines if re.findall(r'[0-9]', l.text)]\n",
    "    print(\"    Time required to retrieve the text lines containing decimals: {:.2f}s\\n\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "time_commentary_operations('RawCommentary')\n",
    "time_commentary_operations('CanonicalCommentary')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T13:29:05.538759Z",
     "start_time": "2023-09-03T13:28:50.330840Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. It allows to deal with multiple versions of the text easily, simply by creating new lists of words and mapping text container customly to any list for any word range (Recall how complicated such an implementation would be if it was to be performed in a nested architecture at line or word level)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Post-processing OCR outputs\n",
    "\n",
    "Now, how does this solves the OCR related issues mentionned above ? These are dealt with in post-processing. `RawCommentary.to_canonical()` therefore launches two operations under the hood:\n",
    "1. Post-processing OCR.\n",
    "2. Converting to `CanonicalCommentary`.\n",
    "\n",
    "Since we already covered the second step, let us briefly go through the first one. Post-processing the OCR aims at harmonizing text, bounding boxes and relations between text containers. It therefore brings a solution to each of the problems listed above:\n",
    "- It deletes empty words and non words.\n",
    "- It adjusts word bounding boxes to their content using contours detection (`cv2.findContours`)\n",
    "- It adjusts line and regions boxes to the minimal box containing the words (for regions, a `_inclusion_threshold` is used, which, set to 0.8 proves to be quiet robust.\n",
    "- It cuts lines according to regions, so that overlapping lines are now chunked.\n",
    "- It removes empty lines\n",
    "- It resets reading order from the region level downwards (i.e. order regions, then line in each region, then words in each line). The algorithm is also robust. Use `AjmcImage.draw_reading_order` to test.\n",
    "\n",
    "All these operations are performed at page level, using `RawPage.optimise()`, which is itself called internally by `RawCommentary.to_canonical()`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "can_commentary = RawCommentary.from_ajmc_data(ocr_path=vs.get_comm_ocr_outputs_dir(comm_id, ocr_run_id).to_canonical()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exporting canonical commentaries to json\n",
    "\n",
    "This last step is pretty straightforward:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "can_commentary.to_json(output_path=None)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If `output_path` is not provided, the canonical json will be automatically exported to the location determined by ajmc's directory structure guidelines (i.e. `/root_dir/comm_id/canonical/v2/ocr_run_id.json`). Under the hood, this calls on each `CanonicalTextContainer`s' specific `self.to_json()` method."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
